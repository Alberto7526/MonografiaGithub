{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import data\n",
    "import metrics\n",
    "import app\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(*, model, metric, X_train, y_train, X_test, y_test,dataset):\n",
    "    train_predictions = model.predict(X_train)\n",
    "    test_predictions = model.predict(X_test)\n",
    "    train_error = metric(y_train, train_predictions)\n",
    "    test_error = metric(y_test, test_predictions)\n",
    "    return {\n",
    "        \"train_predictions\": train_predictions,\n",
    "        \"test_predictions\": test_predictions,\n",
    "        \"train_error\": train_error,\n",
    "        \"test_error\": test_error\n",
    "    }\n",
    "\n",
    "def evaluate_model_baseline(*, model, metric, X_train, y_train, X_test, y_test,dataset):\n",
    "    train_predictions = model.predict(X_train)\n",
    "    test_predictions = model.predict(X_test)\n",
    "    train_error = metric(y_train, train_predictions,X_train)\n",
    "    test_error = metric(y_test, test_predictions,X_test)\n",
    "    return {\n",
    "        \"train_predictions\": train_predictions,\n",
    "        \"test_predictions\": test_predictions,\n",
    "        \"train_error\": train_error,\n",
    "        \"test_error\": test_error\n",
    "    }\n",
    "\n",
    "def print_report(*, model, evaluation):\n",
    "    print(f\"Model used:\\n\\t{model}\")\n",
    "    print(f\"Error:\\n\\ttrain set {evaluation['train_error']}\\n\\tTest error: {evaluation['test_error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = app._get_dataset({'use_new_data': 'si',\n",
    "                            'filepath': \"./Datasets/Sales_train.csv\",\n",
    "                            'filepath_new_data': \"./NewDataset/New_dataset.csv\",\n",
    "                            'process': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used:\n",
      "\tPipeline(steps=[('baseline',\n",
      "                 <model.SalesPerCategory object at 0x0000025873C2E1C0>)])\n",
      "Error:\n",
      "\ttrain set {'cnt_error': 854165.3511036371, 'total_money': 741121379.3659881}\n",
      "\tTest error: {'cnt_error': 363387.532712415, 'total_money': 545070801.7491974}\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(\"models\", \"Baseline\", \"model.joblib\")\n",
    "model = joblib.load(model_path)\n",
    "evaluation = evaluate_model_baseline(\n",
    "    model=model,\n",
    "    metric=metrics.custom_error,\n",
    "    X_train=dataset[\"train\"][0],\n",
    "    y_train=dataset[\"train\"][1],\n",
    "    X_test=dataset[\"test\"][0],\n",
    "    y_test=dataset[\"test\"][1],\n",
    "    dataset = dataset\n",
    ")\n",
    "print_report(model=model, evaluation=evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = app._get_dataset({'use_new_data': 'si',\n",
    "                            'filepath': \"./Datasets/Sales_train.csv\",\n",
    "                            'filepath_new_data': \"./NewDataset/New_dataset.csv\",\n",
    "                            'process': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used:\n",
      "\tPipeline(steps=[('logistic-regressor', LogisticRegression(random_state=0))])\n",
      "Error:\n",
      "\ttrain set 0.30032137903974937\n",
      "\tTest error: 0.34464922376235907\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(\"models\", \"Logistic_Regression\", \"model.joblib\")\n",
    "model = joblib.load(model_path)\n",
    "evaluation = evaluate_model(\n",
    "    model=model,\n",
    "    metric=metrics.mean_absolute_error,\n",
    "    X_train=dataset[\"train\"][0],\n",
    "    y_train=dataset[\"train\"][1],\n",
    "    X_test=dataset[\"test\"][0],\n",
    "    y_test=dataset[\"test\"][1],\n",
    "    dataset = dataset\n",
    ")\n",
    "print_report(model=model, evaluation=evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gridsearchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame([[1,2,3],[2,3,4],[3,4,5],[4,5,6],[5,6,7],[6,7,8],[7,8,9],[8,9,10],[9,10,11],[10,11,12]])\n",
    "y = pd.DataFrame([1,2,3,4,5,6,7,8,9,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockingTimeSeriesSplit():\n",
    "    def __init__(self, n_splits):\n",
    "        self.n_splits = n_splits\n",
    "    \n",
    "    def get_n_splits(self, X, y, groups):\n",
    "        return self.n_splits\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        k_fold_size = n_samples // self.n_splits\n",
    "        indices = np.arange(n_samples)\n",
    "\n",
    "        margin = 0\n",
    "        for i in range(self.n_splits):\n",
    "            start = i * k_fold_size\n",
    "            stop = start + k_fold_size\n",
    "            mid = int(0.8 * (stop - start)) + start\n",
    "            yield indices[start: mid], indices[mid + margin: stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'yield' outside function (<ipython-input-11-812fd1b005e7>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-812fd1b005e7>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    yield indices[0:2]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'yield' outside function\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(10)\n",
    "yield indices[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "model = LogisticRegression()\n",
    "btscv = BlockingTimeSeriesSplit(n_splits=5)\n",
    "scores = cross_val_score(model, x,y, cv=btscv, scoring='r2')\n",
    "print(\"Loss: {0:.3f} (+/- {1:.3f})\".format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
